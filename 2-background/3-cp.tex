\documentclass[../Document.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}


\section{Constraint Programming}
\label{sec:intro/cp}
\acrlong{cp} is a complete, heuristic guided search method which excels at ensuring the respect of constraints while generating a solution. It is complete in that if a solution exists in a given search space, a \cp model is guaranteed to find it. By using heuristics as well as constraint propagation (more on that later), it can be much faster than a simple brute force of all possible solutions.

% We will first define how a simple \cp model functions: its definition, the declaration of constraints, the constraint propagation, the branching decisions, error detection and management.
We will first define how a simple \cp model functions. We will cover the initial problem declaration, the constraint declaration to describe the problem and finally the solving process and its intricacies (constraint propagation, branching decisions, backtracking).
Once that is covered, we can expand on this topic by introducing \gls{cpbp}~\cite{GP:BP}
which is an improvement over standard constraint propagation and leads to more informed decisions. We use \gls{cpbp} in our work since it tends to yield better results and allows for the combination with a \gls{ml} model as we will describe later.

\subsection{Used Constraints}

\subsubsection{All Different Constraint}
The \alldifferent constraint takes a single parameter:
\begin{itemize}
    \item $X$: a subset of variables.
\end{itemize}

It does exactly as its name implies and ensures that the variables within its scope are each assigned a different value.

$$
    \alldifferent(\langle X_1, X_2, \ldots, X_n \rangle)
$$

\subsubsection{Among Constraint}
The \among constraint takes three parameters: 
\begin{itemize}
    \item $X$: a subset of variables,
    \item $V$: a set of values whose occurrences we count in the variables $X$,
    \item $o$: a set of allowed occurrences. This can be a simple integer when only a single value is allowed.
\end{itemize}

It ensures the values in $V$ appear as many times as one of the values in $o$.
For the sake of our example, we define $o$ as a set containing 3 values: $o_1, o_2, o_3$.

$$
    \among(\langle X_1, X_2, \ldots, X_n \rangle, \{V_1, V_2, \ldots, V_m\}, \{o_1, o_2, o_3\})
$$

\subsubsection{Element Constraint}
The \element constraint takes three parameters:
\begin{itemize}
    \item $V$: an array of size $m$,
    \item $X$: a variable whose domain has $m$ as an upper bound,
    \item $Y$: a variable which will be constrained to take a value from $V$.
\end{itemize}

It binds the value of $Y$ to the value in $V$ indexed by the value of $X$. In other words, if $X$ had a value of $i$, then we would attempt to assign $V_i$ to $Y$.

$$
    \element([V_1, V_2, \ldots, V_i, \ldots, V_m], X, Y)
$$

\subsubsection{Grammar Constraint}
The \grammar constraint takes two parameters:
\begin{itemize}
    \item $X$: a subset of variables,
    \item $g$: a \acrfull{cfg}.
\end{itemize}

It ensures that the values assigned to the variables represent a word from the given grammar's recognized language.

This constraint is critical to our work as it allows us to represent our molecule encoding as a \gls{cfg}, guaranteeing that generated sequences are a word in the recognized language.

$$
    \grammar(\langle X_1, X_2, \ldots, X_n \rangle, g)
$$

\subsubsection{Regular Constraint}
The \regular constraint usually takes four parameters:
\begin{itemize}
    \item $X$: a subset of variables,
    \item $\mathcal{A}$: a transition matrix mapping each state to the next appropriate state given an input value. For a given state, $s$, and value $v$, the next state, $s'$, is determined as follows: $A[s][v] = s'$,
    \item $S$: an initial state,
    \item $f$: a list of final states. This parameter can be omitted in the case where all states are valid accepting states, which is the case in the \regular constraints used in our work.
\end{itemize}

Starting at the initial state, we use the value of the next variable (\ie the first one) in the sequence to map towards the next state. The constraint is respected if our last variable leads to an accepting state.

$$
    \regular(\langle X_1, X_2, \ldots, X_n \rangle, \mathcal{A}, S, [s_1, s_3, \ldots])
$$

\subsubsection{Cost Regular Constraint}
The \costregular constraint takes the same four parameters as the \regular constraint, however it takes two supplementary values:
\begin{itemize}
    \item $\mathcal{W}$: A matrix indicating the associated weight to each state transition in $\mathcal{A}$,
    \item $t$: A variable whose domain constrains the minimal and maximal allowed cost.
\end{itemize}

Similarly to the \regular constraint, we go through the state machine using the value of each variable to determine the next state to visit.
However, we now have two conditions to respect. First, we still have to end on a valid accepting state as was the case previously. Second, our final cost must be in the domain of our total cost variable, $t$.
We represent the domain of the $t$ variable underneath the \costregular constraint. For the sake of our example, we define $t^-$ and $t^+$ as the minimal and maximal value respectively.

\begin{align*}
    &\costregular(\langle X_1, X_2, \ldots, X_n \rangle, \mathcal{A}, \mathcal{W}, S, [s_1, s_3, \ldots], t)\\
    &t^- \leq t \leq t^+
\end{align*}

\subsubsection{Sum Constraint}
The \somme constraint takes two parameters:
\begin{itemize}
    \item $X$: a subset of variables,
    \item $Y$: a variable that represents the sum of the values of variables $X$.
\end{itemize}

This constraint constrains the sum of the values of the variables in its scope to be within the domain of the variable $Y$.
We represent the domain of the $Y$ variable underneath the \somme constraint. For the sake of our example, we define $y^-$ and $y^+$ as the minimal and maximal value respectively for variable $Y$.

\begin{align*}
    &\somme(\langle X_1, X_2, \ldots, X_n \rangle, Y)\\
    &y^- \leq Y \leq y^+
\end{align*}

\subsubsection{Table Constraint}
The \extensional constraint takes two parameters:
\begin{itemize}
    \item $X$: a subset of variables,
    \item $\mathcal{T}$: a table of recognized tuples.
\end{itemize}

The \extensional constraint ensures that the given variables are assigned values such that there is a matching tuple in the table $\mathcal{T}$.

$$
    \extensional(\langle X_1, X_2, \ldots, X_n \rangle, \mathcal{T})
$$

\subsubsection{Short Table Constraint}
The \shortTable is an improvement on the \extensional constraint and takes the same two parameters.
It allows the use of a wild card token in the table.
We use this to compress a very large table in our work (Chapter~\ref{chap:cp-validity}).


%%% SubSection Base Model Definition %%%
\subsection{Constraint Satisfaction Problem}
A \csp is defined in three parts:
\begin{itemize}
    \item The variables making up the problem, defined as the finite set $\mathcal{X}$
    \item The domains of these variables, defined as a finite set of values $D$. Each variable can have its own domain
    \item The constraints, each of which is applied to a subset of the variables, defined as a set of constraints $C$.
\end{itemize}

There are a finite number of \textbf{variables} defined in the set $\mathcal{X}$. Each of these variables has its own \textbf{domain} as is defined in the set $D$, which contains the possible values that a variable may take on. Finally, we define a finite number of \textbf{constraints}, each of which is applied on a subset of the variables. Each variable must then be assigned a value from its domain such that it respects all the applied constraints. If such an assignment is possible for all the variables, that is a solution to the problem.

If we take the Sudoku problem as an example, a classic and very commonly seen problem, we can define it as a \csp as follows.
Our \textbf{variables} will be each tile in the 9x9 grid. While this gives us the layout of our problem, we must define the possible values for each variable to be able to solve this problem.
All the variables can take on the same values and so we can define the \textbf{domain} as being the integer values between 1 and 9 inclusively. 

We could represent this using a 2-dimensional array of variables like so:

\begin{align}
    tile[i][j] \in \{1,2,\dots,9\} \mid i,j \in \{1,2,\dots,9\}
\end{align}

All that is missing are the constraints, which are the source of the complexity of the problem.

The \textbf{constraints} in a Sudoku are fairly straightforward, lines, columns and all 3x3 sub-grids within the total grid may not contain any repeat values.
In the \cp community, this type of constraint is very common and is called an \alldifferent constraint. The Sudoku problem would therefore have the following constraints:

\begin{gather*}
    \alldifferent(tile[1][j], tile[2][j], \dots, tile[9][j])\ \forall\ j \mid j \in \{1,2,\dots,9\} \\
    \alldifferent(tile[i][1], tile[i][2], \dots, tile[i][9])\ \forall\ i \mid i \in \{1,2,\dots,9\} \\
    \alldifferent(\\
        tile[3u+1][3v+1],tile[3u+1][3v+2],tile[3u+1][3v+3],\\
        tile[3u+2][3v+1],tile[3u+2][3v+2],tile[3u+2][3v+3],\\
        tile[3u+3][3v+1],tile[3u+3][3v+2],tile[3u+3][3v+3],\\
    )\ \forall\ u,v \mid u,v \in \{0,1,2\}
\end{gather*}

Overall, we would need 81 variables to define this \csp as well as 27 constraints. Each of our variables could take on any of the 9 possible values in their domain.

%%% SubSection Domain Filtering %%%
\subsection{Domain Filtering}
As mentioned in the previous section, each constraint is applied to a subset of the variables in the problem definition. When a constraint is declared, a filtering algorithm that is specific to that constraint will eliminate values that are inconsistent.

The simple example below illustrates how a constraint can filter a variable's domain after being declared.

\begin{gather*}
    x \in \{2,3,4\}\\
    y \in \{1,2,3\}\\
    x \leq y\\
    x \in \{2,3,\bcancel{4}\}\\
    y \in \{\bcancel{1},2,3\}
\end{gather*}

Both variables initially contained a value that would always breach the constraint if chosen. A value such as that one is said to have no support, \ie there are no solutions to the current constraint that contain this value. A visual representation of this can be seen in Figure~\ref{fig:domain_filtering}, where a constraint is applied to two different variables and both have their domain filtered. While we do not know all the solutions to a problem in all cases, we can use logical processes to determine values that would guarantee a breach of the constraint.


\begin{figure}[ht]
    \centering
    \input{figures/fig:domain_filtering.tex}
    \caption[Domain filtering on one variable.]{A constraint is declared on both variable $X$ and $Y$. Valid combinations to the constraint are illustrated by green points on the grid. Values that have no support (no solution to this constraint contains these values) are removed from the domain of the variable. This can be seen as a projection of the solution space onto the domain of each variable.}
    \label{fig:domain_filtering}
\end{figure} 

%%% SubSection Constraint Propagation %%%
\subsection{Constraint Propagation}
Now that we have declared our constraints, the solver begins propagating the consequences of these constraints. Each constraint in the queue communicates to the variables it affects which values in the domain have to be filtered out. Once a variable's domain has been changed, it notifies the constraints that are affected by the change and those constraints are then added to the queue again.

The solver continues propagating the consequences of the constraints and updating domains until it reaches one of three situations: 
\begin{enumerate}
    \item The queue is empty, but there remain unassigned variables.
    \item All variables have been assigned a value, this is a solution to the problem.
    \item One of the variables' domain has been completely filtered, there is no solution in the current state of the problem.
\end{enumerate}

In the first case, there is nothing else to deduce with the information currently available and the solver has to make a branching decision from the current state. Any time we reach one of the three cases above, we can consider that state as being a node in the search tree. The solver makes a branching decision from the current node and propagates the consequences of this decision until it reaches another node to handle.

In the second case, the solver has found a solution and can add it it the solution set. Once the solution has been found, we backtrack to the previous node in the search tree and search along the other branches.

Finally, if we reach an unsatisfiable state, the solver backtracks to the previous node and continues its search from there.

Since we have a finite number of values, we know that this process will eventually end and we will either find a value that respects the constraint, or, find that the constraint cannot be satisfied.

To continue with the example of a Sudoku, a classic way humans continue solving, once they reach a dead end in their reasoning, is by assigning a value to a tile and seeing if they reach a contradictory state.
If they do, then they know their choice was wrong and they can eliminate that possibility.


%%% SubSection Marginal %%%
\subsection{Marginals-Augmented Constraint Programming}
In an ideal world, if we knew every possible solution to a problem, we could use the values within the solution to inform our search and avoid bad branching decisions. This is especially useful when we consider bigger problems that might have a huge combinatorial space to explore.

Marginals-augmented constraint programming is the idea of guiding our branching decisions by counting the number of solutions to a constraint that contain a given value for a given variable as seen in Figure~\ref{fig:marginal_cp}. The difficulty of this task is that it requires an efficient algorithm which can predict the number of solutions without finding and enumerating all possible solutions to the constraint.

\begin{figure}[ht]
    \centering
    \input{figures/fig:marginal_cp.tex}
    \caption[Marginal-Augmented \acrlong{cp}.]{Taking the same example as in Figure~\ref{fig:domain_filtering}, we can count how many valid solutions contain a given value for both variables $X$ and $Y$. The numbers seen on the left and the bottom of the grid indicate the number of solutions containing the value. This can help guide our branching decisions towards solution-dense regions in the search space.}
    \label{fig:marginal_cp}
\end{figure}

One use of these marginals is to change standard constraint propagation to contain more information. \bp does this by modifying the message that constraints send variables. Instead of sending a message containing a binary representation of which values in the domain have a support, the messages are modified to communicate the probability of a value being contained within a solution as seen in Figure~\ref{fig:cpbp_messaging}. This allows the solver to avoid branching on values that have a very small chance of being valid.




\begin{figure}[ht]
    \centering
    \vspace{0.5cm}
    \vbox to 4cm {
        \input{figures/fig:cpbp_messaging.tex}
    }
    \caption[\acrlong{cpbp} messaging.]{\bp replaces standard constraint messages, which consist of a binary message indicating which values are supported in the domain, with a probabilistic distribution over the domain. As we can see, instead of communicating that the value $c$ for variable $X$ lacks a support, the blue constraint communicates that $X=c$ has a $0\%$ chance of being in a valid solution. This ensures that we can still communicate what values must be filtered out, but we also gain information on the other values in the domain.}
    \label{fig:cpbp_messaging}
\end{figure}

When multiple constraints interact on one variable, they each simultaneously communicate to the variable what they estimate the probability distribution to be. The variable then merges these probabilities into the final values as seen in Figure~\ref{fig:cpbp_combined}.


\begin{figure}[ht]
    \centering
    \input{figures/fig:cpbp_combined.tex}
    \caption[\acrlong{cpbp} combining probabilities]{The variables which are affected by multiple constraints ($X$ and $Z$) merge the communicated probabilities that were communicated by the different constraints. $X$ filters out the value $c$ and $Z$ filters out $b$, as was communicated by the constraints seen in Figure~\ref{fig:cpbp_messaging}.}
    \label{fig:cpbp_combined}
\end{figure}


\end{document}