\documentclass[../Document.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}


\Chapter{LITERATURE REVIEW}
\label{chap:lit-review}

This chapter will focus on an overview of the current state of the art on this topic of research.

\subsection{Drug Discovery}
Drug Discovery, and molecule design in general, is a vast topic.
There are many different methods that are applicable to the problem.

A recent survey by Du et al.~\cite{du2022molgensurvey} presents various representation formalisms. It covers one-dimensional representations such as \acrshort{smiles} and \acrshort{inchi} as well as two-dimensional and three-dimensional representations.
It describes some of the main problems tackled, and an array of computational methods used to solve them, mostly generative machine learning but also combinatorial solvers.
They mention the difficulty of exploring little known/seen areas of the molecular design space (the common out-of-distribution generation issue) and the need for lots of training data (generation in low-data regime issue i.e. high sample complexity).
They also mention as opportunity the generation of specialized molecules with more complex structure.

Since our work focuses on this subject, the next sections will give more about interesting methods and their applications to drug discovery.


\section{\acrshort{nlp} applied to drug discovery}
\label{sec:lit-review/nlp}
There are many other works that have applied \gls{nlp} to the domain of drug discovery.

GÃ³mez-Bombarelli et al.~\cite{gomez-bombarelli_automatic_2018} work on converting molecules from a \smiles representation to a latent space from which properties can be estimated. An encoder encodes \smiles inputs into a latent space, a multilayer perceptron model is used to estimate properties from a point in this latent space and an encoder can take a point in the latent space and parse it back out into a \smiles string.
They also tested their work on an \gls{inchi} representation, but results were unsatisfactory due to the notation's complexity. 

Schoenmaker et al.~\cite{schoenmaker_uncorrupt_2023} propose a \smiles string corrector. This technique would take invalid outputs from a previous model and fix them. When given an invalid molecule and its valid pair, the model was generally capable of getting the right value. While we also wish to address the invalid molecules generated by a model, we wish to do so at inference time, and we hope to add property targeting.

Vidal et al.~\cite{lingos} worked on a technique directly inspired from \gls{nlp}: n-grams. While they do not use this to generate molecules, they report a decent predictive model using a linear regression, capable of estimating certain molecular properties. We believe this work might be relevant to ours as we also wish to estimate the same property, however we wish to do this directly in our \cp model.

Bagal et al.~\cite{bagal_molgpt_2022} describe different methods that have been gaining traction in the field of drug discovery, datasets that they use for training and some properties they train their model to predict. Their model is capable of generating molecules with a given structure as well as given properties.

We end by mentioning the work of Guo et al.~\cite{guo2022dataefficient} who recently proposed a sample-efficient neural method for molecule generation that is based on learning a graph grammar. This is similar to the method we wish to apply, by using the \grammar constraint.


\section{\acrshort{cp} applied to drug discovery}
\label{sec:lit-review/cp}
Among combinatorial solvers, the use of constraint programming in this area was pioneered 25 years ago by Krippahl and Barahona for protein structure determination~\cite{DBLP:conf/cp/KrippahlB99}.
They showed that \cp can help determine the position of atoms in a molecule.
By approximating the distance between non-hydrogen atoms they infer the shape of the protein.

Later work on protein docking~\cite{DBLP:journals/almob/KrippahlB15} uses \cp to prune the search space, allowing a trained Naive Bayes classifier to find solutions much faster.

Several works consider a particular family of molecules, benzenoids, and exploit their special geometry when defining their representation in a \cp model and expressing various properties as constraints.
Carissan et al.~\cite{DBLP:journals/constraints/CarissanHPTV22,DBLP:journals/jcisd/VaretPTHC22} add constraints to benzenoid generation in order to model certain properties such as the number of carbon atoms or the shape of the molecule.
They also formulate the problem of determining local aromaticity as a \csp.
Peng and Solnon~\cite{DBLP:conf/cp/PengS23} improve the enumeration of benzenoid graphs by representing them using short canonical codes that are invariant to symmetries and rotations, expressed in a \cp model.
They ensure the presence of a given pattern by completing a suitably prefixed code.
The sequential nature of these codes, obtained through graph traversal, makes them similar in spirit to the SMILES notation, though much less general.

In the context of their work on constrained graph generation using \cp, Omrani and\\ Naanaa~\cite{DBLP:journals/constraints/OmraniN20} consider the generation of molecular graphs corresponding to a given molecular formula.

So despite some prior work involving \cp, none address the problem we consider and especially the use of the \grammar constraint.


\subsection{Combining \acrshort{cp} with \acrshort{ml}}
\label{sec:lit-review/gpt+cp}
Combining \cp with \gls{ml} has been a subject of interest for a while.
One way to achieve this is by combining the constraints directly into the reward signal. The resulting sequences are more likely to avoid undesirable traits while still reflecting the training data and achieving good sample diversity.

By expressing arbitrary constraint in the \cpbp framework and automatically deriving a reward signal, both Lafleur et al.~\cite{DBLP:conf/cp/LafleurCP22} and Yin et al.~\cite{DBLP:conf/cpaior/YinCP24} manage to improve this.

In a \cp-driven generation, \cite{DBLP:conf/cp/ReginMB24} builds a \csp incrementally by adding sequence variables on the fly and limiting their domain to an LLM's short list of candidate tokens, queried at each step.

Deutsch et al.~\cite{deutsch2019general} express commonly-used constraints in \gls{nlp} as automata. At each step, the automata filter out the inconsistent token values and renormalize the \nn's output probability distribution accordingly.

Our method is closest in spirit to this last source as we also modify the output probability distribution by removing inconsistent values. However, more importantly, we potentially change these probabilities relative to each other to reflect the marginal probabilities computed from the constraints of our \cp model.

Using CP also offers a variety of constraints, including automata~\cite{DBLP:conf/cp/Pesant04}, and inconsistent values are removed at each step through constraint propagation instead of checking the full completion of the partial sequence with the automata.

However, the field that interests is of more interest to us, is the addition of a constraint module post-training to a \nn, as this resembles our method more.

Lattner et al.~\cite{lattner2018imposing} use a convolutional restricted Boltzmann machine as a generative model and enforce constraints as differentiable cost functions that are minimized during the sampling process to resemble the structure of a reference musical piece.

Lee et al.~\cite{lee2019gradient} use gradient-based inference to continue adjusting the model's parameters toward the satisfaction of the constraints during inference.

Paolo et al.~\cite{DBLP:conf/nesy/DragoneTP21} introduce a \emph{constrained structured predictor} expressed in a \cp language that acts as a final layer to a \nn and which is trainable to finetune the predictions but also enforces the constraints during inference.

\end{document}