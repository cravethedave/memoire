\documentclass[../Document.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}


\Chapter{LITERATURE REVIEW}
\label{chap:lit-review}

This chapter will focus on an overview of the state of the art on this topic of research.

\section{Drug Discovery}
Drug Discovery, and molecule design in general, is a vast topic.
There are many different methods that are applicable to the problem.

A recent survey by Du et al.~\cite{du2022molgensurvey} presents various representation formalisms. It covers one-dimensional representations such as \acrshort{smiles} and \acrshort{inchi} as well as two-dimensional and three-dimensional representations.
It describes some of the main problems tackled, and an array of computational methods used to solve them, mostly generative machine learning but also combinatorial solvers.
They mention the difficulty of exploring little known/seen areas of the molecular design space (the common out-of-distribution generation issue) and the need for lots of training data (generation in low-data regime issue i.e. high sample complexity).
They also mention as opportunity the generation of specialized molecules with more complex structure.

Since our work focuses on this subject, the next sections will give more about interesting methods and their applications to drug discovery.


\section{\acrshort{nlp} applied to drug discovery}
\label{sec:lit-review/nlp}
There are many works that have applied \gls{nlp} to the domain of drug discovery. They use a one-dimensional representation, as was discussed previously, to represent molecules as words, allowing standard \gls{nlp} methods to be used.

GÃ³mez-Bombarelli et al.~\cite{gomez-bombarelli_automatic_2018} work on converting molecules from a \smiles representation to a latent space from which properties can be estimated. An encoder encodes \smiles inputs into a latent space, a multilayer perceptron model is used to estimate properties from a point in this latent space and a decoder can take a point in the latent space and parse it back out into a \smiles string.
This allows them to generate new molecules with desirable properties by: decoding random vectors in their latent space, altering a known molecule's representation or combining parts of different known molecule encodings into a new one.
Furthermore, by representing molecules as a latent space, they can use known techniques to search through a continuous representation while looking for molecules that match the desired properties.
One such technique is gradient-based optimization.
They also test their work using an \gls{inchi} representation, but results were unsatisfactory due to the notation's complexity.
However, this work limits itself to molecules which contain fewer than nine non-Hydrogen atoms.

Schoenmaker et al.~\cite{schoenmaker_uncorrupt_2023} propose a \smiles string corrector. This technique would take invalid outputs from a previous model and fix them. When given an invalid molecule and its valid representation (\ie what the molecule should be after it is fixed), the model was generally capable of getting the right value. While we also wish to address the invalid molecules generated by a model, we wish to do so at inference time, and we hope to add property targeting.

Vidal et al.~\cite{lingos} worked on a technique directly inspired from \gls{nlp}: n-grams.
This technique allows models to learn more context by grouping tokens into what we call n-grams.
We first group every sequence of two individual tokens together and add to our language the encountered $2$-grams.
This process can then be repeated with any sequence of three individual tokens for $3$-grams and so on for any given integer.
This process can be repeated until we are satisfied with the context our model learns.
While they do not use this to generate molecules, they report a decent predictive model using a linear regression, capable of estimating certain molecular properties. We believe this work might be relevant to ours as we also wish to estimate the same property, however we wish to do this directly in our \cp model.

Bagal et al.~\cite{bagal_molgpt_2022} describe different methods that have been gaining traction in the field of drug discovery, datasets that they use for training and some properties they train their model to predict.
They train their model to generate full molecules when given target properties and an initial scaffold.
They use RDKit\footnote{https://www.rdkit.org/}, an open source tool, to calculate a molecule's properties and extract the \gls{smiles} scaffold to use as an initial sequence.
Their model is also capable of targeting specific structure.
While the model trains to predict the \emph{logP} score of a molecule, a property we wish to model later on in Chapter~\ref{chap:cp-validity}, it also targets properties that fall out of scope of our work.

We end by mentioning the work of Guo et al.~\cite{guo2022dataefficient} who recently proposed a sample-efficient neural method for molecule generation that is based on learning a graph grammar. This is similar to the method we wish to apply, by using the \grammar constraint.


\section{\acrshort{cp} applied to drug discovery}
\label{sec:lit-review/cp}
Among combinatorial solvers, the use of constraint programming in this area was pioneered 25 years ago by Krippahl and Barahona for protein structure determination~\cite{DBLP:conf/cp/KrippahlB99}.
They showed that \cp can help determine the position of atoms in a molecule.
By approximating the distance between non-hydrogen atoms they infer the shape of the protein.

Later work on protein docking~\cite{DBLP:journals/almob/KrippahlB15} uses \cp to prune the search space, allowing a trained Naive Bayes classifier to find solutions much faster.

Several works consider a particular family of molecules, benzenoids, and exploit their special geometry when defining their representation in a \cp model and expressing various properties as constraints.
Carissan et al.~\cite{DBLP:journals/constraints/CarissanHPTV22,DBLP:journals/jcisd/VaretPTHC22} add constraints to benzenoid generation in order to model certain properties such as the number of carbon atoms or the shape of the molecule.
They also formulate the problem of determining local aromaticity as a \csp.
Peng and Solnon~\cite{DBLP:conf/cp/PengS23} improve the enumeration of benzenoid graphs by representing them using short canonical codes that are invariant to symmetries and rotations, expressed in a \cp model.
They ensure the presence of a given pattern by completing a suitably prefixed code.
The sequential nature of these codes, obtained through graph traversal, makes them similar in spirit to the SMILES notation, though much less general.

In the context of their work on constrained graph generation using \cp, Omrani and\\ Naanaa~\cite{DBLP:journals/constraints/OmraniN20} consider the generation of molecular graphs corresponding to a given molecular formula.

So despite some prior work involving \cp, none address the problem we consider and especially the use of the \grammar constraint.


\section{Combining \acrshort{cp} with \acrshort{ml}}
\label{sec:lit-review/gpt+cp}
Combining \cp with \gls{ml} has been a subject of interest for a while.
Specifically in the field of Reinforcement Learning, where constraints can be directly injected into the reward signal~\cite{DBLP:conf/cp/LafleurCP22,DBLP:conf/cpaior/YinCP24}.
This would train the model to respect the constraints, though still gives no guarantee that it will be respected.
Resulting sequences are more likely to avoid undesirable traits while still reflecting the training data and achieving good sample diversity.

However, the field that is of more interest to us is the addition of a constraint module post-training to a \nn, as this resembles our method more.

Lattner et al.~\cite{lattner2018imposing} use a convolutional restricted Boltzmann machine as a generative model and enforce constraints as differentiable cost functions that are minimized during the sampling process to resemble the structure of a reference musical piece.
By representing constraints as differentiable cost functions, they show that they can integrate the constraints to the musical generation while maintaining the learned local musical coherence of the original model.

Lee et al.~\cite{lee2019gradient} use gradient-based inference to adjust the model's parameters toward the satisfaction of the constraints during inference.
They show that by modifying the feed-forward of their model, they can give weight to constraints.
This modifies the model's inner workings and guides the generation towards valid sequences as they demonstrate in their work.

While these past methods did integrate constraints into their \gls{ml} model at inference time, they are not using \cp and use different representations.
These next methods combine the two using a \csp as we intend to do.

Paolo et al.~\cite{DBLP:conf/nesy/DragoneTP21} introduce a \emph{constrained structured predictor} expressed in a \cp language that acts as a final layer to a \nn.
The total model can be trained to finetune the predictions while enforcing soft and hard constraints during inference.
They then compare their hybrid architecture, {\sc Nester} to both separate models and find that achieves better results.
While this work uses \cp to refine a \gls{ml} model, it acts on the output of the latter model.
Note, this model is not used to generate, it was used to predict handwriting.

In a \cp-driven generation, as we intend to do, \cite{DBLP:conf/cp/ReginMB24} builds a \csp incrementally by adding sequence variables on the fly and limiting their domain to an LLM's short list of candidate tokens, queried at each step.
Similarly to what we wish to do in our work, they query a \gls{llm} model at each step of the sequence generation and use the most probable returned values to determine the potential values for the next token.
The main difference between us and this work is that we will modify the probabilities of the next token returned by the \gls{llm} by using \bp.

While these are closer to our work, we plan to use \cp as a way to modify the message of the \gls{ml} model as we generate the sequence token-by-token.

Deutsch et al.~\cite{deutsch2019general} do something extremely similar. By expressing commonly-used constraints in \gls{nlp} as automata, they filter out inconsistent token values and renormalize the \nn's output probability distribution accordingly at each step of the generation.
This approach allows them to apply constraint at inference time to guarantee the respect of given constraints.
Our method is closest in spirit to this as we also modify the output probability distribution by removing inconsistent values at inference time.
However, more importantly, we potentially change these probabilities relative to each other to reflect the marginal probabilities computed from the constraints of our \cp model.
Guiding the model towards valid solutions while maintaining the \gls{ml} model's message.
It remains to be seen how well this works.

Using CP also offers a variety of constraints, including automata~\cite{DBLP:conf/cp/Pesant04}, allowing inconsistent values to be removed at each step through constraint propagation instead of checking the full completion of the partial sequence with the automata.

\end{document}